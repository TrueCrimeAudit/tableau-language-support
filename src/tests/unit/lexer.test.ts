// src/tests/unit/lexer.test.ts\n\nimport { tokenize, TokenType } from '../../lexer';\n\ndescribe('Lexer', () => {\n    describe('Basic Tokenization', () => {\n        it('should tokenize simple function calls', () => {\n            const tokens = tokenize('SUM([Sales])');\n            \n            expect(tokens).toBeDefined();\n            expect(tokens.length).toBeGreaterThan(0);\n            \n            // Should have function name, parentheses, field reference\n            const tokenTypes = tokens.map(t => t.type);\n            expect(tokenTypes).toContain(TokenType.Identifier); // SUM\n            expect(tokenTypes).toContain(TokenType.LParen);     // (\n            expect(tokenTypes).toContain(TokenType.LBracket);   // [\n            expect(tokenTypes).toContain(TokenType.RBracket);   // ]\n            expect(tokenTypes).toContain(TokenType.RParen);     // )\n        });\n        \n        it('should tokenize field references', () => {\n            const tokens = tokenize('[Sales]');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(3); // [, Sales, ]\n            expect(tokens[0].type).toBe(TokenType.LBracket);\n            expect(tokens[1].type).toBe(TokenType.Identifier);\n            expect(tokens[1].value).toBe('Sales');\n            expect(tokens[2].type).toBe(TokenType.RBracket);\n        });\n        \n        it('should tokenize field references with spaces', () => {\n            const tokens = tokenize('[Customer Name]');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(3);\n            expect(tokens[0].type).toBe(TokenType.LBracket);\n            expect(tokens[tokens.length - 2].value).toContain('Customer');\n            expect(tokens[tokens.length - 1].type).toBe(TokenType.RBracket);\n        });\n        \n        it('should tokenize string literals', () => {\n            const tokens = tokenize('\"Hello World\"');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(1);\n            const stringToken = tokens.find(t => t.type === TokenType.String);\n            expect(stringToken).toBeDefined();\n            expect(stringToken?.value).toBe('\"Hello World\"');\n        });\n        \n        it('should tokenize single-quoted strings', () => {\n            const tokens = tokenize('\\'Hello World\\'');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(1);\n            const stringToken = tokens.find(t => t.type === TokenType.String);\n            expect(stringToken).toBeDefined();\n            expect(stringToken?.value).toBe('\\'Hello World\\'');\n        });\n        \n        it('should tokenize numeric literals', () => {\n            const tokens = tokenize('123.45');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(1);\n            const numberToken = tokens.find(t => t.type === TokenType.Number);\n            expect(numberToken).toBeDefined();\n            expect(numberToken?.value).toBe('123.45');\n        });\n        \n        it('should tokenize integer literals', () => {\n            const tokens = tokenize('42');\n            \n            expect(tokens.length).toBeGreaterThanOrEqual(1);\n            const numberToken = tokens.find(t => t.type === TokenType.Number);\n            expect(numberToken).toBeDefined();\n            expect(numberToken?.value).toBe('42');\n        });\n    });\n    \n    describe('Operator Tokenization', () => {\n        it('should tokenize arithmetic operators', () => {\n            const tokens = tokenize('+ - * / %');\n            \n            const operatorTokens = tokens.filter(t => \n                t.type === TokenType.Plus ||\n                t.type === TokenType.Minus ||\n                t.type === TokenType.Multiply ||\n                t.type === TokenType.Divide ||\n                t.type === TokenType.Modulo\n            );\n            \n            expect(operatorTokens.length).toBe(5);\n        });\n        \n        it('should tokenize comparison operators', () => {\n            const tokens = tokenize('> < >= <= = <>');\n            \n            const comparisonTokens = tokens.filter(t => \n                t.type === TokenType.Greater ||\n                t.type === TokenType.Less ||\n                t.type === TokenType.GreaterEqual ||\n                t.type === TokenType.LessEqual ||\n                t.type === TokenType.Equal ||\n                t.type === TokenType.NotEqual\n            );\n            \n            expect(comparisonTokens.length).toBe(6);\n        });\n        \n        it('should tokenize logical operators', () => {\n            const tokens = tokenize('AND OR NOT');\n            \n            const logicalTokens = tokens.filter(t => \n                t.type === TokenType.And ||\n                t.type === TokenType.Or ||\n                t.type === TokenType.Not\n            );\n            \n            expect(logicalTokens.length).toBe(3);\n        });\n    });\n    \n    describe('Keyword Tokenization', () => {\n        it('should tokenize IF/THEN/ELSE/END keywords', () => {\n            const tokens = tokenize('IF THEN ELSE ELSEIF END');\n            \n            const keywordTokens = tokens.filter(t => \n                t.type === TokenType.If ||\n                t.type === TokenType.Then ||\n                t.type === TokenType.Else ||\n                t.type === TokenType.ElseIf ||\n                t.type === TokenType.End\n            );\n            \n            expect(keywordTokens.length).toBe(5);\n        });\n        \n        it('should tokenize CASE/WHEN keywords', () => {\n            const tokens = tokenize('CASE WHEN');\n            \n            const caseTokens = tokens.filter(t => \n                t.type === TokenType.Case ||\n                t.type === TokenType.When\n            );\n            \n            expect(caseTokens.length).toBe(2);\n        });\n        \n        it('should tokenize LOD keywords', () => {\n            const tokens = tokenize('FIXED INCLUDE EXCLUDE');\n            \n            const lodTokens = tokens.filter(t => \n                t.type === TokenType.Fixed ||\n                t.type === TokenType.Include ||\n                t.type === TokenType.Exclude\n            );\n            \n            expect(lodTokens.length).toBe(3);\n        });\n    });\n    \n    describe('Delimiter Tokenization', () => {\n        it('should tokenize parentheses', () => {\n            const tokens = tokenize('()');\n            \n            expect(tokens.some(t => t.type === TokenType.LParen)).toBe(true);\n            expect(tokens.some(t => t.type === TokenType.RParen)).toBe(true);\n        });\n        \n        it('should tokenize brackets', () => {\n            const tokens = tokenize('[]');\n            \n            expect(tokens.some(t => t.type === TokenType.LBracket)).toBe(true);\n            expect(tokens.some(t => t.type === TokenType.RBracket)).toBe(true);\n        });\n        \n        it('should tokenize braces', () => {\n            const tokens = tokenize('{}');\n            \n            expect(tokens.some(t => t.type === TokenType.LBrace)).toBe(true);\n            expect(tokens.some(t => t.type === TokenType.RBrace)).toBe(true);\n        });\n        \n        it('should tokenize commas and semicolons', () => {\n            const tokens = tokenize(',;');\n            \n            expect(tokens.some(t => t.type === TokenType.Comma)).toBe(true);\n            expect(tokens.some(t => t.type === TokenType.Semicolon)).toBe(true);\n        });\n    });\n    \n    describe('Complex Expression Tokenization', () => {\n        it('should tokenize IF expressions', () => {\n            const tokens = tokenize('IF [Sales] > 100 THEN \"High\" ELSE \"Low\" END');\n            \n            expect(tokens.length).toBeGreaterThan(10);\n            \n            // Should contain all expected token types\n            const tokenTypes = tokens.map(t => t.type);\n            expect(tokenTypes).toContain(TokenType.If);\n            expect(tokenTypes).toContain(TokenType.Then);\n            expect(tokenTypes).toContain(TokenType.Else);\n            expect(tokenTypes).toContain(TokenType.End);\n            expect(tokenTypes).toContain(TokenType.Greater);\n        });\n        \n        it('should tokenize CASE expressions', () => {\n            const tokens = tokenize('CASE [Category] WHEN \"Furniture\" THEN 1 ELSE 0 END');\n            \n            const tokenTypes = tokens.map(t => t.type);\n            expect(tokenTypes).toContain(TokenType.Case);\n            expect(tokenTypes).toContain(TokenType.When);\n            expect(tokenTypes).toContain(TokenType.Then);\n            expect(tokenTypes).toContain(TokenType.Else);\n            expect(tokenTypes).toContain(TokenType.End);\n        });\n        \n        it('should tokenize LOD expressions', () => {\n            const tokens = tokenize('{ FIXED [Region] : SUM([Sales]) }');\n            \n            const tokenTypes = tokens.map(t => t.type);\n            expect(tokenTypes).toContain(TokenType.LBrace);\n            expect(tokenTypes).toContain(TokenType.Fixed);\n            expect(tokenTypes).toContain(TokenType.Colon);\n            expect(tokenTypes).toContain(TokenType.RBrace);\n        });\n        \n        it('should tokenize nested function calls', () => {\n            const tokens = tokenize('SUM(AVG([Sales]))');\n            \n            const identifiers = tokens.filter(t => t.type === TokenType.Identifier);\n            expect(identifiers.length).toBeGreaterThanOrEqual(3); // SUM, AVG, Sales\n            \n            const parens = tokens.filter(t => \n                t.type === TokenType.LParen || t.type === TokenType.RParen\n            );\n            expect(parens.length).toBe(4); // Two opening, two closing\n        });\n    });\n    \n    describe('Whitespace and Comments', () => {\n        it('should handle whitespace correctly', () => {\n            const tokens = tokenize('SUM ( [Sales] )');\n            \n            // Whitespace should be handled but not necessarily tokenized\n            const nonWhitespaceTokens = tokens.filter(t => t.type !== TokenType.Whitespace);\n            expect(nonWhitespaceTokens.length).toBeGreaterThan(0);\n        });\n        \n        it('should tokenize single-line comments', () => {\n            const tokens = tokenize('SUM([Sales]) // This is a comment');\n            \n            const commentToken = tokens.find(t => t.type === TokenType.Comment);\n            expect(commentToken).toBeDefined();\n            expect(commentToken?.value).toContain('This is a comment');\n        });\n        \n        it('should tokenize multi-line comments', () => {\n            const tokens = tokenize('SUM([Sales]) /* Multi\\nline\\ncomment */');\n            \n            const commentToken = tokens.find(t => t.type === TokenType.Comment);\n            expect(commentToken).toBeDefined();\n            expect(commentToken?.value).toContain('Multi');\n        });\n    });\n    \n    describe('Position Information', () => {\n        it('should provide accurate line and column information', () => {\n            const tokens = tokenize('SUM([Sales])');\n            \n            tokens.forEach(token => {\n                expect(token.line).toBeGreaterThanOrEqual(0);\n                expect(token.column).toBeGreaterThanOrEqual(0);\n            });\n            \n            // First token should be at line 0, column 0\n            expect(tokens[0].line).toBe(0);\n            expect(tokens[0].column).toBe(0);\n        });\n        \n        it('should handle multi-line input correctly', () => {\n            const tokens = tokenize('SUM([Sales])\\nAVG([Profit])');\n            \n            const secondLineTokens = tokens.filter(t => t.line === 1);\n            expect(secondLineTokens.length).toBeGreaterThan(0);\n            \n            // First token on second line should be at column 0\n            const firstSecondLineToken = secondLineTokens[0];\n            expect(firstSecondLineToken.column).toBe(0);\n        });\n    });\n    \n    describe('Error Handling', () => {\n        it('should handle empty input', () => {\n            const tokens = tokenize('');\n            \n            expect(tokens).toBeDefined();\n            expect(Array.isArray(tokens)).toBe(true);\n            // Should have at least EOF token\n            expect(tokens.length).toBeGreaterThanOrEqual(1);\n            expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);\n        });\n        \n        it('should handle whitespace-only input', () => {\n            const tokens = tokenize('   \\n  \\t  ');\n            \n            expect(tokens).toBeDefined();\n            expect(Array.isArray(tokens)).toBe(true);\n        });\n        \n        it('should handle invalid characters gracefully', () => {\n            const tokens = tokenize('SUM([Sales]) @#$');\n            \n            expect(tokens).toBeDefined();\n            expect(Array.isArray(tokens)).toBe(true);\n            \n            // Should still tokenize the valid parts\n            const identifiers = tokens.filter(t => t.type === TokenType.Identifier);\n            expect(identifiers.some(t => t.value === 'SUM')).toBe(true);\n            expect(identifiers.some(t => t.value === 'Sales')).toBe(true);\n        });\n        \n        it('should handle unclosed strings', () => {\n            const tokens = tokenize('\"Unclosed string');\n            \n            expect(tokens).toBeDefined();\n            expect(Array.isArray(tokens)).toBe(true);\n            \n            // Should handle gracefully, possibly with error token\n            const stringTokens = tokens.filter(t => \n                t.type === TokenType.String || t.type === TokenType.Error\n            );\n            expect(stringTokens.length).toBeGreaterThan(0);\n        });\n        \n        it('should handle unmatched delimiters', () => {\n            const tokens = tokenize('SUM([Sales]');\n            \n            expect(tokens).toBeDefined();\n            expect(Array.isArray(tokens)).toBe(true);\n            \n            // Should still tokenize what it can\n            const identifiers = tokens.filter(t => t.type === TokenType.Identifier);\n            expect(identifiers.some(t => t.value === 'SUM')).toBe(true);\n        });\n    });\n    \n    describe('Performance', () => {\n        it('should tokenize quickly for normal input', () => {\n            const input = 'IF [Sales] > 100 THEN SUM([Profit]) ELSE AVG([Discount]) END';\n            \n            const startTime = Date.now();\n            const tokens = tokenize(input);\n            const duration = Date.now() - startTime;\n            \n            expect(tokens).toBeDefined();\n            expect(duration).toBeLessThan(50); // Should be very fast\n        });\n        \n        it('should handle large input efficiently', () => {\n            const largeInput = Array.from({ length: 1000 }, (_, i) => \n                `SUM([Field${i}])`\n            ).join(' + ');\n            \n            const startTime = Date.now();\n            const tokens = tokenize(largeInput);\n            const duration = Date.now() - startTime;\n            \n            expect(tokens).toBeDefined();\n            expect(duration).toBeLessThan(500); // Should handle large input\n        });\n    });\n    \n    describe('Token Properties', () => {\n        it('should provide complete token information', () => {\n            const tokens = tokenize('SUM([Sales])');\n            \n            tokens.forEach(token => {\n                expect(token).toHaveProperty('type');\n                expect(token).toHaveProperty('value');\n                expect(token).toHaveProperty('line');\n                expect(token).toHaveProperty('column');\n                \n                expect(typeof token.type).toBe('number');\n                expect(typeof token.value).toBe('string');\n                expect(typeof token.line).toBe('number');\n                expect(typeof token.column).toBe('number');\n            });\n        });\n        \n        it('should have EOF token at the end', () => {\n            const tokens = tokenize('SUM([Sales])');\n            \n            expect(tokens.length).toBeGreaterThan(0);\n            expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);\n        });\n        \n        it('should maintain token order', () => {\n            const tokens = tokenize('SUM([Sales])');\n            \n            // Tokens should be in order of appearance\n            let lastPosition = -1;\n            tokens.forEach(token => {\n                const position = token.line * 1000 + token.column;\n                expect(position).toBeGreaterThanOrEqual(lastPosition);\n                lastPosition = position;\n            });\n        });\n    });\n});\n"